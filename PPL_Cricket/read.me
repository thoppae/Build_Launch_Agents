Tech Stack:

Streamlit: Used for creating the web application interface.
Python: The primary programming language.
LangChain: Used for building the Language Model application, specifically for the Retrieval-Augmented Generation (RAG) part.
Google Generative AI (Gemini): The underlying large language model and embedding model used.
FAISS: Used as the vector store for efficient similarity search of documents.

Libraries Used:

os: For accessing environment variables (like the API key).
streamlit: For building the web UI.
csv: For reading data from CSV files.
dotenv: For loading environment variables from a .env file.
langchain: Core LangChain library.
langchain_core.documents: For creating Document objects.
langchain_community.vectorstores.FAISS: The FAISS vector store implementation.
langchain_google_genai: LangChain integration with Google Generative AI.
nest_asyncio: Used to patch asyncio for compatibility, likely within the Streamlit environment.

Code Details:

Setup and Configuration:
Loads environment variables using dotenv.
Retrieves the Google API key.
Configures the Streamlit page.
Initializes the ChatGoogleGenerativeAI (Gemini 1.5 Flash) and GoogleGenerativeAIEmbeddings models, caching them for efficiency.
Data Loading and Processing:
Includes a helper function _clean_csv_dict to clean keys from CSV rows.
create_combined_vectordb function:
Reads player data from data.csv.
Creates a main document for each player with all their details.
Improvement: Creates additional, focused "expert" documents for key attributes like "Committee Role" and "Injury Status" to potentially improve retrieval accuracy for specific questions.
Reads team selection rules from rules.csv.
Combines all player and rule documents.
Uses FAISS.from_documents to create a vector database from the documents and embeddings. This database is also cached.

LangChain Components:
get_qa_chain function:
Defines a PromptTemplate for factual Q&A.
Creates a RetrievalQA chain using the initialized LLM and the FAISS vector database as a retriever.
get_full_context_from_files function:
Reads the raw content of rules.csv and data.csv and formats them into strings. This is used for the more complex team formation task where the entire context is provided to the LLM.

Streamlit UI:
Presents a section for factual Q&A:
A text input for the question.
A submit button.
Displays the answer from the qa_chain and optionally shows the source documents.
Presents a section for advanced team formation:
A number input to specify how many teams to form.
A button to trigger team generation.
When the button is clicked, it retrieves the full context of rules and players, constructs a detailed prompt for the LLM, and displays the LLM's response (which includes the reasoning and the final teams).

Next Steps (as implemented in the code):

The code is designed to be run as a Streamlit application.
The next step is typically to run this Python script using the streamlit run command.
The user would then interact with the web interface to ask questions or generate teams.
Final Steps (as intended by the code):

The final output for the user is either:
An answer to their factual question, potentially with sources.
The generated team compositions and the LLM's reasoning process for forming those teams based on the provided rules and player data.
